{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #can delete thsi when have sosas csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ?\n",
    "# check if can have log loss for training and if it's ok to do it like this for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_report(model, X_train, X_test, y_train, y_test, name):\n",
    "    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision'))\n",
    "    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='roc_auc'))\n",
    "\n",
    "    df_model_train = pd.DataFrame({'data'        : 'training',\n",
    "                             'model'        : [name],\n",
    "                             'accuracy'     : [accuracy],\n",
    "                             'precision'    : [precision],\n",
    "                             'recall'       : [recall],\n",
    "                             'f1score'      : [f1score],\n",
    "                             'rocauc'       : [rocauc],\n",
    "                             'logloss'      : 'N/A'})\n",
    "    \n",
    "    accuracy     = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision    = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    y_pred = model.predict(X_test)\n",
    "    logloss      = log_loss(y_test, y_pred)   # SVC & LinearSVC unable to use cvs\n",
    "\n",
    "    df_model_test = pd.DataFrame({'data'        : 'test',\n",
    "                             'model'        : [name],\n",
    "                             'accuracy'     : [accuracy],\n",
    "                             'precision'    : [precision],\n",
    "                             'recall'       : [recall],\n",
    "                             'f1score'      : [f1score],\n",
    "                             'rocauc'       : [rocauc],\n",
    "                             'logloss'      : [logloss]})   # timetaken: to be used for comparison later\n",
    "    \n",
    "    df_model = pd.concat([df_model_train, df_model_test])\n",
    "\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing clean dataframe (with NaNs filled)\n",
    "\n",
    "#df = pd.read_csv('data/xx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### To delete this step when have the dataframe from sosa\n",
    "\n",
    "# since don't have yeat the real df, will upload the old one\n",
    "\n",
    "df = pd.read_csv('data/clean_tf.csv')\n",
    "\n",
    "df = df.fillna(method='ffill')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "rand = [random.random() for i in range(len(df))]\n",
    "df['random'] = rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if imported data is correct\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the types of the variables\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO:\n",
    "Make function that works with different train-test, different scaling, and different balancing\n",
    "Output: Descriptive dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're encoding categorical variables to be able to use them in the models of the machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Label Encoder for the columns Revenue and Weekend because these columns have only two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Assigning the columns\n",
    "cols_label_enc = ['Revenue', 'Weekend']\n",
    "\n",
    "# Creating labels for the columns\n",
    "for col in cols_label_enc:\n",
    "    df[col+'_enc'] = labelencoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T16:29:39.088671Z",
     "start_time": "2020-05-06T16:29:39.085695Z"
    }
   },
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using OneHotEncoder for Month, OperatingSystems, Region, TrafficType and Browser columns because they have more than two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_dfs = []\n",
    "\n",
    "columns = ['Month', 'OperatingSystems', 'Region', 'Browser', 'TrafficType']\n",
    "\n",
    "for col in columns:\n",
    "    # Creating instance of one-hot-encoder\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    # Passing columns\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(df[[col]]).toarray())\n",
    "    # Getting values for the names of the columns\n",
    "    enc_df.columns = enc.get_feature_names()+col\n",
    "    # Appenign all new dfs to one list\n",
    "    appended_dfs.append(enc_df)\n",
    "# Mergeing with the main df\n",
    "df_encoded = df.join(appended_dfs) \n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the columns that were encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['Revenue', 'Weekend', 'Month', 'OperatingSystems', 'Region', 'Browser', 'TrafficType', 'VisitorType']\n",
    "df_encoded.drop(columns=cols_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the data is encoded correctly\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting encoded data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv(\"data/df_encoded.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data to: <br> X - independent variables <br> y - dependent variable (outcome) <br><br>Eliminating PageValues from the independent variables because it is too dependent on the outcome, it will not be possible to use it in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns=['Revenue_enc', 'PageValues'])\n",
    "y = df_encoded['Revenue_enc']\n",
    "\n",
    "print(f'Original dataset shape X: {len(X)}, y: {len(y)}')\n",
    "print(f'Original split between True and False:\\n{y.value_counts()}')\n",
    "\n",
    "sns.countplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing (Undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As identified in the EDA in the previous step, we have a problem of our data being imbalanced. We will use the Undersample method which is the most conveniant for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explain why NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = NearMiss()\n",
    "X, y = nr.fit_sample(X, y)\n",
    "\n",
    "print(f'Resampled dataset shape X: {len(X)}, Y: {len(y)}')\n",
    "print(f'Resampled split between True and False:\\n{pd.Series(y).value_counts()}')\n",
    "\n",
    "sns.countplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the train and test data in X training values and y target column.\n",
    "\n",
    "- With shuffle, we set whether or not to shuffle the data before splitting (Default True).\n",
    "- With stratify, we choose to split the data stritifying via labels (Default None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling allows us to standarize the numerical values of our dataset, centering to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "\n",
    "X_train = std_scale.fit_transform(X_train)\n",
    "\n",
    "X_test = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neighbors = 3\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = neighbors)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "knn_results = baseline_report(knn, X_train, X_test, y_train, y_test, 'KNeighborsClassifier')\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "logreg_results = baseline_report(logreg, X_train, X_test, y_train, y_test, 'LogisticRegression')\n",
    "logreg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "\n",
    "dectree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = dectree.predict(X_test)\n",
    "y_pred_train = dectree.predict(X_train)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "dectree_results = baseline_report(dectree, X_train, X_test, y_train, y_test, 'DecisionTreeClassifier')\n",
    "dectree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(dectree, df_encoded)\n",
    "plt.savefig('feature_importance_dectree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfor = RandomForestClassifier()\n",
    "rfor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = rfor.predict(X_test)\n",
    "y_pred_train = rfor.predict(X_train)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "rfor_results = baseline_report(rfor, X_train, X_test, y_train, y_test, 'RandomForestClassifier')\n",
    "rfor_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're defining a function to visualize the most important features defined by Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ??\n",
    "\n",
    "# Should I move this to functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, df):\n",
    "    names = df.columns[model.feature_importances_.argsort()]\n",
    "    model.feature_importances_.sort()\n",
    "    plt.figure(figsize=(15,15))\n",
    "    n_features = len(names)\n",
    "    plt.barh(range(n_features), np.sort(model.feature_importances_), align='center')\n",
    "    plt.yticks(np.arange(n_features), names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(rfor, df_encoded)\n",
    "plt.savefig('feature_importance_rfor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results\n",
    "\n",
    "# can choose to split where we see elbow or after random\n",
    "\n",
    "#strange that anypage has such a low performance, while each page alone have higher.\n",
    "#check if it's done ok, if it's not redundant info\n",
    "#check what we put under browser 99 when did transformation, why other has higher importance??\n",
    "\n",
    "#explanation of alberto, to change\n",
    "\"\"\"\n",
    "Similarly to the single decision tree, the random forest also gives a lot of importance to the “Glucose” feature, but it also chooses “BMI” to be the 2nd most informative feature overall. The randomness in building the random forest forces the algorithm to consider many possible explanations, the result being that the random forest captures a much broader picture of the data than a single tree.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what other options for grid have or other options for hyperparameter tuning\n",
    "# Should I pass all parameters to grid or just some??\n",
    "\n",
    "param_grid = {'n_estimators': [200, 500],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth' : [4,5,6,7,8],\n",
    "              'criterion' :['gini', 'entropy']}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, n_jobs= 1)\n",
    "                  \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with best parameters\n",
    "\n",
    "baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same cross validation done manually, check the difference with function\n",
    "\n",
    "#Cross validation with best_estimator from grid search (using strat_k_fold, by default have kfold)\n",
    "\n",
    "strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "initial_score = cross_val_score(grid.best_estimator_, X_test, y_test, cv=strat_k_fold, scoring='f1').mean()\n",
    "print(\"Final accuracy : {} \".format(initial_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miltiple scoring, should give the same as function, check the difference in settings\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "initial_score = cross_validate(grid.best_estimator_, X_test, y_test, cv=strat_k_fold, scoring=('f1', 'accuracy'))\n",
    "print(initial_score['test_f1'].mean())\n",
    "print(initial_score['test_accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linearsvc = LinearSVC()\n",
    "linearsvc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = linearsvc.predict(X_train)\n",
    "y_pred_test = linearsvc.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "linearsvc_results = baseline_report(linearsvc, X_train, X_test, y_train, y_test, 'LinearSVC')\n",
    "linearsvc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T15:38:30.059489Z",
     "start_time": "2020-05-06T15:38:30.057214Z"
    }
   },
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_b = GaussianNB()\n",
    "naive_b.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = naive_b.predict(X_train)\n",
    "y_pred_test = naive_b.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "naive_b_results = baseline_report(naive_b, X_train, X_test, y_train, y_test, 'GaussianNB')\n",
    "naive_b_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
