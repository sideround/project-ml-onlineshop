{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random ################can delete thsi when have sosas csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_report(model, X_train, X_test, y_train, y_test, name):\n",
    "    \"\"\"\n",
    "    The function takes the model, the split data and the name and returns the dataframe with\n",
    "    the scores of the models with training and test data.\n",
    "    \"\"\"\n",
    "    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision'))\n",
    "    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    logloss      = log_loss(y_train, y_pred_train)\n",
    "\n",
    "    df_model_train = pd.DataFrame({'data'        : 'training',\n",
    "                             'model'        : [name],\n",
    "                             'accuracy'     : [accuracy],\n",
    "                             'precision'    : [precision],\n",
    "                             'recall'       : [recall],\n",
    "                             'f1score'      : [f1score],\n",
    "                             'rocauc'       : [rocauc],\n",
    "                             'logloss'      : [logloss]})\n",
    "    \n",
    "    accuracy     = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision    = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    logloss      = log_loss(y_test, y_pred_test)   # SVC & LinearSVC unable to use cvs\n",
    "\n",
    "    df_model_test = pd.DataFrame({'data'        : 'test',\n",
    "                             'model'        : [name],\n",
    "                             'accuracy'     : [accuracy],\n",
    "                             'precision'    : [precision],\n",
    "                             'recall'       : [recall],\n",
    "                             'f1score'      : [f1score],\n",
    "                             'rocauc'       : [rocauc],\n",
    "                             'logloss'      : [logloss]})   # timetaken: to be used for comparison later\n",
    "    \n",
    "    df_model = pd.concat([df_model_train, df_model_test], ignore_index=True)\n",
    "\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, df):\n",
    "    \"\"\"\n",
    "    The function takes the model and the dataframe and returns graph showing the importance of\n",
    "    the features.\n",
    "    \"\"\"\n",
    "    names = df.columns[model.feature_importances_.argsort()]\n",
    "    model.feature_importances_.sort()\n",
    "    plt.figure(figsize=(15,15))\n",
    "    n_features = len(names)\n",
    "    plt.barh(range(n_features), np.sort(model.feature_importances_), align='center')\n",
    "    plt.yticks(np.arange(n_features), names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_defaut(main_model_results, compare_model_results_hyp):\n",
    "    \"\"\"\n",
    "    Function takes main model results with default values and model to compare results after the\n",
    "    hyperparameter tuning and returns the comparison of the results.\n",
    "    \"\"\"\n",
    "    print(f'{main_model_results.iloc[1][\"model\"]} is giving us the F1 score of {round(main_model_results.iloc[1][\"f1score\"], 3)} on the test data.')\n",
    "\n",
    "    if main_model_results.iloc[1][\"f1score\"] > compare_model_results_hyp.iloc[1][\"f1score\"]:\n",
    "          print(f'{main_model_results.iloc[1][\"model\"]} improves the result from {compare_model_results_hyp.iloc[1][\"model\"]} (score: {round(compare_model_results_hyp.iloc[1][\"f1score\"], 3)})')\n",
    "    else:\n",
    "          print(f'{main_model_results.iloc[1][\"model\"]} does not imporve the score from {compare_model_results_hyp.iloc[1][\"model\"]} (score: {round(compare_model_results_hyp.iloc[1][\"f1score\"], 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_after_hyp(main_model_results_hyp, main_model_results, compare_model_results_hyp):\n",
    "    \"\"\"\n",
    "    Function takes main model results before and after the hyperparameter tuning \n",
    "    and model to compare results after the hyperparameter tuning \n",
    "    and returns the comparison of the results.\n",
    "    \"\"\"\n",
    "    print(f'{main_model_results_hyp.iloc[1][\"model\"]} is giving us the F1 score of {round(main_model_results_hyp.iloc[1][\"f1score\"], 3)} on the test data.')\n",
    "\n",
    "    if main_model_results_hyp.iloc[1][\"f1score\"] > main_model_results.iloc[1][\"f1score\"]:\n",
    "        print(f'Hyperparameter tuning imporved our score from {round(main_model_results.iloc[1][\"f1score\"], 3)} to {round(main_model_results_hyp.iloc[1][\"f1score\"], 3)}')\n",
    "    else:\n",
    "        print(f'Our score after the hyperparameter tuning changed from {round(main_model_results.iloc[1][\"f1score\"], 3)} to {round(main_model_results_hyp.iloc[1][\"f1score\"], 3)}')\n",
    "\n",
    "    if main_model_results_hyp.iloc[1][\"f1score\"] > compare_model_results_hyp.iloc[1][\"f1score\"]:\n",
    "          print(f'{main_model_results_hyp.iloc[1][\"model\"]} improves the result from {compare_model_results_hyp.iloc[1][\"model\"]} (score: {round(compare_model_results_hyp.iloc[1][\"f1score\"], 3)})')\n",
    "    else:\n",
    "          print(f'{main_model_results_hyp.iloc[1][\"model\"]} does not imporve the score from {compare_model_results_hyp.iloc[1][\"model\"]} (score: {round(compare_model_results_hyp.iloc[1][\"f1score\"], 3)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be working with the clean dataframe (errors and NaN values were handled in the previous step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading clean dataframe\n",
    "\n",
    "#df = pd.read_csv('data/xx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# To delete this step when have the dataframe from sosa\n",
    "\n",
    "# since don't have yeat the real df, will upload the old one\n",
    "\n",
    "df = pd.read_csv('data/clean_tf.csv')\n",
    "\n",
    "df = df.fillna(method='ffill')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "rand = [random.random() for i in range(len(df))]\n",
    "df['random'] = rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating data to see what changes have to be done before moving to the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if imported data is correct\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the types of the variables\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this short investigation, we have identified that there are several categorical variables that will need to be encoded before using data with the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're encoding categorical variables to be able to use them in the models of machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Label Encoder for the columns 'Revenue' and 'Weekend'. These columns have only two categories meaning that we don't need to create different columns to encode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Assigning the columns\n",
    "cols_label_enc = ['Revenue', 'Weekend']\n",
    "\n",
    "# Creating labels for the columns\n",
    "for col in cols_label_enc:\n",
    "    df[col+'_enc'] = labelencoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Encoded Revenue values:\\n{df.Revenue_enc.value_counts()}')\n",
    "print(f'\\nEncoded Weekend values:\\n{df.Weekend_enc.value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the value counts of these columns we can see that the values have been encoded to 0 (False) and 1 (True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T16:29:39.088671Z",
     "start_time": "2020-05-06T16:29:39.085695Z"
    }
   },
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using OneHotEncoder for 'Month', 'OperatingSystems', 'Region', 'TrafficType' and 'Browser' columns because they have more than two categories, therefore, additional columns have to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_dfs = []\n",
    "\n",
    "columns = ['Month', 'OperatingSystems', 'Region', 'Browser', 'TrafficType']\n",
    "\n",
    "for col in columns:\n",
    "    # Creating instance of one-hot-encoder\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    # Passing columns\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(df[[col]]).toarray())\n",
    "    # Getting values for the names of the columns\n",
    "    enc_df.columns = enc.get_feature_names()+col\n",
    "    # Appenign all new dfs to one list\n",
    "    appended_dfs.append(enc_df)\n",
    "# Mergeing with the main df\n",
    "df_encoded = df.join(appended_dfs) \n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the columns that were encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['Revenue', 'Weekend', 'Month', 'OperatingSystems', 'Region', 'Browser', 'TrafficType', \n",
    "             'VisitorType']\n",
    "df_encoded.drop(columns=cols_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the data has been encoded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of the encoded dataset is: {df_encoded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting encoded data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv(\"data/df_encoded.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use data with the machine learning models we will need to split it to the independent variables (X) and target variable (y). \n",
    "\n",
    "We are also eliminating 'PageValues' from the independent variables because (as identified in the previous steps) it is too dependent on the outcome and it will not be possible to use it in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns=['Revenue_enc', 'PageValues'])\n",
    "y = df_encoded['Revenue_enc']\n",
    "\n",
    "print(f'Original dataset shape X: {len(X)}, y: {len(y)}')\n",
    "print(f'\\nOriginal split between True and False:\\n{y.value_counts()}')\n",
    "\n",
    "sns.countplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing (Undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As identified in the EDA in the previous step, we have imbalanced data distribution, the observations in the class False are much higher than the observations in the class True.\n",
    "\n",
    "Standard ML techniques such as Decision Tree and Logistic Regression have a bias towards the majority class, and they tend to ignore the minority class. They tend only to predict the majority class, hence, having major misclassification of the minority class in comparison with the majority class.\n",
    "\n",
    "We will use the NearMiss, an undersampling technique for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:48.834187Z",
     "start_time": "2020-05-08T10:11:48.831194Z"
    }
   },
   "source": [
    "This technique aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, it removes the instances of the majority class to increase the spaces between the two classes. This helps in the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = NearMiss()\n",
    "X, y = nr.fit_sample(X, y)\n",
    "\n",
    "print(f'Resampled dataset shape X: {len(X)}, Y: {len(y)}')\n",
    "print(f'\\nResampled split between True and False:\\n{pd.Series(y).value_counts()}')\n",
    "\n",
    "sns.countplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling is a technique to standardize the independent variables present in the data in a fixed range. We will perform it to handle highly varying values of our dataset. If feature scaling is not done, a machine learning algorithm tends to weigh greater values, and consider smaller values as the lower values, regardless of the unit of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the StandardScaler. It is a technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "\n",
    "X = std_scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to train and test our model on the different sets of the dataset, we will split the data into training and testing parts.\n",
    "\n",
    "- With shuffle, we set to shuffle the data before splitting.\n",
    "- With stratify, we choose to split the data stratifying via our target feature (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, \n",
    "                                                    random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be testing our data with 6 classification models:\n",
    "\n",
    "- K-Nearest Neighbors\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Linear Support Vector Classification\n",
    "- Gaussian Naive Bayes\n",
    "\n",
    "We will focus our analysis to optimize the F1_score. When optimizing for F1_score we take both precision and recall into account. Trying to only optimize recall, the algorithm will predict most examples to belong to the positive class, but that will result in many false positives and, hence, low precision. On the other hand, trying to optimize precision, the model will predict very few examples as positive results (the ones with the highest probability), but recall will be very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:45:33.218399Z",
     "start_time": "2020-05-07T08:45:33.214547Z"
    }
   },
   "source": [
    "K-Nearest Neighbors is an algorithm by which an unclassified data point is classified based on its distance from known points.\n",
    "\n",
    "The way that model decides how to classify the unknown point is by creating a circle with the point as the center. The size of the circle is set by choosing the number of neighbors. This setting does not refer to the actual size of the circle, however, it refers to how many neighboring points are to fall inside of the circle.\n",
    "\n",
    "We will start our modeling with the K-Nearest Neighbors with the default number of neighbors (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "knn_results = baseline_report(knn, X_train, X_test, y_train, y_test, 'KNeighborsClassifier')\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:52.358755Z",
     "start_time": "2020-05-08T10:11:52.343131Z"
    }
   },
   "source": [
    "##### Results with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{knn_results.iloc[1][\"model\"]} is giving us the F1 score of {round(knn_results.iloc[1][\"f1score\"], 3)} on the test data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a GridSearchCV to choose the best parameters for our models. The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. \n",
    "\n",
    "We need to keep in mind that GridSearchCV has a very high computation cost if we want to try a bigger range of parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to mention is that n_jobs is set to -1. By setting n_jobs to -1, we are telling the computer to use all of it’s processors to perform the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**n_neighbors**: int, optional (default = 5). Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "We will be passing only the odd numbers as the possible values for the n_neighbors parameter. If you choose an even number, in the case of a tie vote, the decision on which class to assign will be done randomly when weights is set to uniform. By choosing an odd number, there are no ties.\n",
    "\n",
    "**weights**: str or callable, optional (default = 'uniform'). Weight function used in prediction. Possible values: \n",
    "\n",
    "'uniform' : uniform weights. All points in each neighborhood are weighted equally. \n",
    "\n",
    "'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. \n",
    "\n",
    "[callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. \n",
    "\n",
    "**metric**: string or callable, default 'minkowski'. The distance metric to use for the tree. The default metric is minkowski. We will be testing 'euclidean' metric, which distance function is sqrt(sum((x - y)^2)) and 'manhattan' metric, which function is sum(|x - y|).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': [5,7,9,11,13,15,17,19],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'metric' : ['euclidean', 'manhattan']}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_params_knn = grid.best_params_\n",
    "print(best_params_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best parameters for our model. The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "knn_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'KNeighborsClassifier')\n",
    "knn_results_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:59.288051Z",
     "start_time": "2020-05-08T10:11:59.285059Z"
    }
   },
   "source": [
    "##### Results after the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{knn_results_hyp.iloc[1][\"model\"]} is giving us the F1 score of {round(knn_results_hyp.iloc[1][\"f1score\"], 3)} on the test data.')\n",
    "\n",
    "if knn_results_hyp.iloc[1][\"f1score\"] > knn_results.iloc[1][\"f1score\"]:\n",
    "    print(f'Hyperparameter tuning imporved our score from {round(knn_results.iloc[1][\"f1score\"], 3)} to {round(knn_results_hyp.iloc[1][\"f1score\"], 3)}')\n",
    "else:\n",
    "    print(f'Our score after the hyperparameter tuning changed from {round(knn_results.iloc[1][\"f1score\"], 3)} to {round(knn_results_hyp.iloc[1][\"f1score\"], 3)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T09:23:54.157705Z",
     "start_time": "2020-05-08T09:23:54.148684Z"
    }
   },
   "source": [
    "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "logreg_results = baseline_report(logreg, X_train, X_test, y_train, y_test, 'LogisticRegression')\n",
    "logreg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:52.358755Z",
     "start_time": "2020-05-08T10:11:52.343131Z"
    }
   },
   "source": [
    "##### Results with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defaut(logreg_results, knn_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**C**: float, default=1.0 Inverse of regularization strength. Must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "**penalty**: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'. Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver. If 'none' (not supported by the liblinear solver), no regularization is applied.\n",
    "\n",
    "**solver**: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'. Algorithm to use in the optimization problem. For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones. For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes.\n",
    "\n",
    "‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
    "\n",
    "‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "\n",
    "‘saga’ also supports ‘elasticnet’ penalty\n",
    "\n",
    "‘liblinear’ does not support setting penalty='none'\n",
    "\n",
    "**multi_class**: {‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’. If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = list(10.0**np.arange(-3, 3))\n",
    "\n",
    "param_grid = [{'C': c_values,\n",
    "              'penalty': ['l2'],\n",
    "              'solver' : ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "              'multi_class': ['ovr', 'warn']},\n",
    "              {'C': c_values,\n",
    "              'penalty': ['l1'],\n",
    "              'solver' : ['liblinear', 'saga'],\n",
    "              'multi_class': ['ovr', 'warn']}]\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_params_logreg = grid.best_params_\n",
    "print(best_params_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best parameters for our model. The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "logreg_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'LogisticRegression')\n",
    "logreg_results_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:59.288051Z",
     "start_time": "2020-05-08T10:11:59.285059Z"
    }
   },
   "source": [
    "##### Results after the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_after_hyp(logreg_results_hyp, logreg_results, knn_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees learn how to best split the dataset into smaller and smaller subsets to predict the target value. The condition, or test, is represented as the 'leaf' (node) and the possible outcomes as 'branches' (edges). This splitting process continues until no further gain can be made or a preset rule is met, e.g. the maximum depth of the tree is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = dectree.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "dectree_results = baseline_report(dectree, X_train, X_test, y_train, y_test, 'DecisionTreeClassifier')\n",
    "dectree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:52.358755Z",
     "start_time": "2020-05-08T10:11:52.343131Z"
    }
   },
   "source": [
    "##### Results with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defaut(dectree_results, logreg_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're will be using a defined function to visualize the most important features from the Decision Tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(dectree, df_encoded)\n",
    "plt.savefig('feature_importance_dectree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:12:07.977323Z",
     "start_time": "2020-05-08T10:12:07.973283Z"
    }
   },
   "source": [
    "We can see that the most important feature is SpecialDay, other 4 features that highly impact the performance of the model are: Administrative_Duration, ProductRelated_Duration, ExitRates and Visitor_isReturing. After these 5 features we can see that the impact drops more significantly. \n",
    "\n",
    "The good practice is to remove the insignificant variables and run the model again, to avoid the overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**criterion**: optional (default=”gini”) or Choose attribute selection measure: This parameter allows us to use the different attribute selection measure. Supported criteria are “gini” for the Gini index and “entropy” for the information gain.\n",
    "\n",
    "**splitter**: string, optional (default=”best”) or Split Strategy: This parameter allows us to choose the split strategy. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "\n",
    "**max_depth**: int or None, optional (default=None) or Maximum Depth of a Tree: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = list(np.arange(2, 10))\n",
    "\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'splitter': ['best', 'random'],\n",
    "              'max_depth' : depth}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_dectree = grid.best_params_\n",
    "print(best_params_dectree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best parameters for our model. The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "dectree_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'DecisionTreeClassifier')\n",
    "dectree_results_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:59.288051Z",
     "start_time": "2020-05-08T10:11:59.285059Z"
    }
   },
   "source": [
    "##### Results after the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_after_hyp(dectree_results_hyp, dectree_results, logreg_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests construct many individual decision trees at training. Predictions from all trees are pooled to make the final prediction, the mode of the classes for classification or the mean prediction for regression. As they use a collection of results to make a final decision, they are referred to as Ensemble techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfor = RandomForestClassifier()\n",
    "rfor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = rfor.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "rfor_results = baseline_report(rfor, X_train, X_test, y_train, y_test, 'RandomForestClassifier')\n",
    "rfor_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:52.358755Z",
     "start_time": "2020-05-08T10:11:52.343131Z"
    }
   },
   "source": [
    "##### Results with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defaut(rfor_results, dectree_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(rfor, df_encoded)\n",
    "plt.savefig('feature_importance_rfor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a slight difference in the feature selection from the features selected with Decision Tree. Here the importance of the SpecialDay is a lot lower. \n",
    "\n",
    "The randomness in building the random forest forces the algorithm to consider many possible explanations, the result being that the random forest captures a much broader picture of the data than a single tree.\n",
    "\n",
    "The most important feature from the Random Forest is ProductRelated_Duration. Other 8 important features are: ProductRelated, SpecialDay, ExitRates, Administrative, Administrative_Duration, Bouncerates, InformationalDuration and Visitor_isReturning. \n",
    "\n",
    "We can choose to have a split of the features using the elbow principle or using the random feature. The logic behind the random feature is that: if the feature is less important than the random feature (with random values), it could be safely removed from the selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**n_estimators**: integer, optional (default=100). The number of trees in the forest. The default value of n_estimators changed from 10 to 100 in version 0.22.\n",
    "\n",
    "**max_features**: int, float, string or None, optional (default=”auto”). The number of features to consider when looking for the best split:\n",
    "\n",
    "If int, then consider max_features features at each split.\n",
    "\n",
    "If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "\n",
    "If “auto”, then max_features=sqrt(n_features).\n",
    "\n",
    "If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "\n",
    "If “log2”, then max_features=log2(n_features).\n",
    "\n",
    "If None, then max_features=n_features.\n",
    "\n",
    "Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n",
    "\n",
    "**max_depth**: integer or None, optional (default=None). The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "**criterion**: string, optional (default=”gini”). The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [200, 500],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth' : [4,5,6,7,8],\n",
    "              'criterion' :['gini', 'entropy']}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_rfor = grid.best_params_\n",
    "print(best_params_rfor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best parameters for our model. The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "rfor_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'RandomForestClassifier')\n",
    "rfor_results_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:59.288051Z",
     "start_time": "2020-05-08T10:11:59.285059Z"
    }
   },
   "source": [
    "##### Results after the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_after_hyp(rfor_results_hyp, rfor_results, dectree_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of a Linear SVC (Support Vector Classifier) is to fit to the data provided, returning a 'best fit' hyperplane that divides or categorizes the data. After getting the hyperplane, you can then feed some features to the classifier to see what the 'predicted' class is. It has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples compared to SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linearsvc = LinearSVC()\n",
    "linearsvc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = linearsvc.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "linearsvc_results = baseline_report(linearsvc, X_train, X_test, y_train, y_test, 'LinearSVC')\n",
    "linearsvc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:52.358755Z",
     "start_time": "2020-05-08T10:11:52.343131Z"
    }
   },
   "source": [
    "##### Results with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defaut(linearsvc_results, rfor_results_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing results\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**C**: float, optional (default=1.0). Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
    "\n",
    "**multi_class**: str, ‘ovr’ or ‘crammer_singer’ (default=’ovr’). Determines the multi-class strategy if y contains more than two classes. \n",
    "\n",
    "\"ovr\" trains n_classes one-vs-rest classifiers, while \"crammer_singer\" optimizes a joint objective over all classes. While crammer_singer is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If \"crammer_singer\" is chosen, the options loss, penalty and dual will be ignored.\n",
    "\n",
    "**penalty**: str, ‘l1’ or ‘l2’ (default=’l2’). Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef_ vectors that are sparse.\n",
    "\n",
    "**loss**: str, ‘hinge’ or ‘squared_hinge’ (default=’squared_hinge’). Specifies the loss function. ‘hinge’ is the standard SVM loss (used e.g. by the SVC class) while ‘squared_hinge’ is the square of the hinge loss.\n",
    "\n",
    "**dual**: bool, (default=True). Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = list(10.0**np.arange(-3, 3))\n",
    "\n",
    "param_grid = [{'C': c_values,\n",
    "              'multi_class': ['ovr', 'crammer_singer'],\n",
    "              'penalty': ['l1'],\n",
    "              'loss': ['squared_hinge'],\n",
    "              'dual': [0]},\n",
    "              {'C': c_values,\n",
    "              'multi_class': ['ovr', 'crammer_singer'],\n",
    "              'penalty': ['l2'],\n",
    "              'loss': ['squared_hinge'],\n",
    "              'dual': [1]}]\n",
    "\n",
    "grid = GridSearchCV(LinearSVC(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_linearsvc = grid.best_params_\n",
    "print(best_params_linearsvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best parameters for our model. The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "linearsvc_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'LinearSVC')\n",
    "linearsvc_results_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:59.288051Z",
     "start_time": "2020-05-08T10:11:59.285059Z"
    }
   },
   "source": [
    "##### Results after the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_after_hyp(linearsvc_results_hyp, linearsvc_results, rfor_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian Naive Bayes algorithm is a special type of NB algorithm. It assumes that all the features are following a gaussian distribution i.e, normal distribution and that the value of a particular feature is independent of the value of any other feature, given the class variable. An advantage of naive Bayes is that it only requires a small number of training data to estimate the parameters necessary for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_b = GaussianNB()\n",
    "naive_b.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = naive_b.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "naive_b_results = baseline_report(naive_b, X_train, X_test, y_train, y_test, 'GaussianNB')\n",
    "naive_b_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:52.358755Z",
     "start_time": "2020-05-08T10:11:52.343131Z"
    }
   },
   "source": [
    "##### Results with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defaut(naive_b_results, linearsvc_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**var_smoothing** : float, optional (default=1e-9). Portion of the largest variance of all features that is added to variances for calculation stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = list(10.0**np.arange(-9, -4))\n",
    "\n",
    "param_grid = {'var_smoothing' : smooth}\n",
    "\n",
    "grid = GridSearchCV(GaussianNB(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_naive_b = grid.best_params_\n",
    "print(best_params_naive_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best parameters for our model. The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "naive_b_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'GaussianNB')\n",
    "naive_b_results_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:11:59.288051Z",
     "start_time": "2020-05-08T10:11:59.285059Z"
    }
   },
   "source": [
    "##### Results after the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_after_hyp(naive_b_results_hyp, naive_b_results, linearsvc_results_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of all models before hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_hyp = [knn_results, logreg_results, dectree_results, rfor_results, \n",
    "              linearsvc_results, naive_b_results]\n",
    "\n",
    "final_results_before = pd.concat(before_hyp, ignore_index=True)\n",
    "final_results_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_score_before = final_results_before.loc[final_results_before.data == 'test'].f1score.max()\n",
    "best_test_model_before = final_results_before.loc[(final_results_before.data == 'test') & (final_results_before.f1score == best_test_score_before)].model.max()\n",
    "print(f'The model with the best F1_score with the default parameters is {best_test_model_before}, score: {round(best_test_score_before, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of all models after hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_hyp = [knn_results_hyp, logreg_results_hyp, dectree_results_hyp, rfor_results_hyp, \n",
    "              linearsvc_results_hyp, naive_b_results_hyp]\n",
    "\n",
    "final_results_after = pd.concat(after_hyp, ignore_index=True)\n",
    "final_results_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_score_after = final_results_after.loc[final_results_after.data == 'test'].f1score.max()\n",
    "best_test_model_after = final_results_after.loc[(final_results_after.data == 'test') & (final_results_after.f1score == best_test_score_after)].model.max()\n",
    "print(f'The model with the best F1_score after the hyperparameter tuning is {best_test_model_after}, score: {round(best_test_score_after, 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
