{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #can delete thsi when have sosas csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ?\n",
    "# check if can have log loss for training and if it's ok to do it like this for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_report(model, X_train, X_test, y_train, y_test, name):\n",
    "    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision'))\n",
    "    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='roc_auc'))\n",
    "\n",
    "    df_model_train = pd.DataFrame({'data'        : 'training',\n",
    "                             'model'        : [name],\n",
    "                             'accuracy'     : [accuracy],\n",
    "                             'precision'    : [precision],\n",
    "                             'recall'       : [recall],\n",
    "                             'f1score'      : [f1score],\n",
    "                             'rocauc'       : [rocauc],\n",
    "                             'logloss'      : 'N/A'})\n",
    "    \n",
    "    accuracy     = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision    = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    y_pred = model.predict(X_test)\n",
    "    logloss      = log_loss(y_test, y_pred)   # SVC & LinearSVC unable to use cvs\n",
    "\n",
    "    df_model_test = pd.DataFrame({'data'        : 'test',\n",
    "                             'model'        : [name],\n",
    "                             'accuracy'     : [accuracy],\n",
    "                             'precision'    : [precision],\n",
    "                             'recall'       : [recall],\n",
    "                             'f1score'      : [f1score],\n",
    "                             'rocauc'       : [rocauc],\n",
    "                             'logloss'      : [logloss]})   # timetaken: to be used for comparison later\n",
    "    \n",
    "    df_model = pd.concat([df_model_train, df_model_test])\n",
    "\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, df):\n",
    "    names = df.columns[model.feature_importances_.argsort()]\n",
    "    model.feature_importances_.sort()\n",
    "    plt.figure(figsize=(15,15))\n",
    "    n_features = len(names)\n",
    "    plt.barh(range(n_features), np.sort(model.feature_importances_), align='center')\n",
    "    plt.yticks(np.arange(n_features), names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing clean dataframe (with NaNs filled)\n",
    "\n",
    "#df = pd.read_csv('data/xx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### To delete this step when have the dataframe from sosa\n",
    "\n",
    "# since don't have yeat the real df, will upload the old one\n",
    "\n",
    "df = pd.read_csv('data/clean_tf.csv')\n",
    "\n",
    "df = df.fillna(method='ffill')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "rand = [random.random() for i in range(len(df))]\n",
    "df['random'] = rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if imported data is correct\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the types of the variables\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO:\n",
    "Make function that works with different train-test, different scaling, and different balancing\n",
    "Output: Descriptive dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're encoding categorical variables to be able to use them in the models of the machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Label Encoder for the columns Revenue and Weekend because these columns have only two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Assigning the columns\n",
    "cols_label_enc = ['Revenue', 'Weekend']\n",
    "\n",
    "# Creating labels for the columns\n",
    "for col in cols_label_enc:\n",
    "    df[col+'_enc'] = labelencoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T16:29:39.088671Z",
     "start_time": "2020-05-06T16:29:39.085695Z"
    }
   },
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using OneHotEncoder for Month, OperatingSystems, Region, TrafficType and Browser columns because they have more than two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_dfs = []\n",
    "\n",
    "columns = ['Month', 'OperatingSystems', 'Region', 'Browser', 'TrafficType']\n",
    "\n",
    "for col in columns:\n",
    "    # Creating instance of one-hot-encoder\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    # Passing columns\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(df[[col]]).toarray())\n",
    "    # Getting values for the names of the columns\n",
    "    enc_df.columns = enc.get_feature_names()+col\n",
    "    # Appenign all new dfs to one list\n",
    "    appended_dfs.append(enc_df)\n",
    "# Mergeing with the main df\n",
    "df_encoded = df.join(appended_dfs) \n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the columns that were encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['Revenue', 'Weekend', 'Month', 'OperatingSystems', 'Region', 'Browser', 'TrafficType', 'VisitorType']\n",
    "df_encoded.drop(columns=cols_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the data is encoded correctly\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting encoded data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv(\"data/df_encoded.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data to: <br> X - independent variables <br> y - dependent variable (outcome) <br><br>Eliminating PageValues from the independent variables because it is too dependent on the outcome, it will not be possible to use it in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns=['Revenue_enc', 'PageValues'])\n",
    "y = df_encoded['Revenue_enc']\n",
    "\n",
    "print(f'Original dataset shape X: {len(X)}, y: {len(y)}')\n",
    "print(f'Original split between True and False:\\n{y.value_counts()}')\n",
    "\n",
    "sns.countplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing (Undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As identified in the EDA in the previous step, we have a problem of our data being imbalanced. We will use the Undersample method which is the most conveniant for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explain why NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = NearMiss()\n",
    "X, y = nr.fit_sample(X, y)\n",
    "\n",
    "print(f'Resampled dataset shape X: {len(X)}, Y: {len(y)}')\n",
    "print(f'Resampled split between True and False:\\n{pd.Series(y).value_counts()}')\n",
    "\n",
    "sns.countplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling allows us to standarize the numerical values of our dataset, centering to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "\n",
    "X = std_scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the train and test data in X training values and y target column.\n",
    "\n",
    "- With shuffle, we set whether or not to shuffle the data before splitting (Default True).\n",
    "- With stratify, we choose to split the data stritifying via labels (Default None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:45:33.218399Z",
     "start_time": "2020-05-07T08:45:33.214547Z"
    }
   },
   "source": [
    "K-Nearest Neighbors (kNN) is an algorithm by which an unclassified data point is classified based on it’s distance from known points.\n",
    "\n",
    "The way that model decides how to classify the unknown point is by creating a circle with the point as the center. The size of the circle is set by choosing the number of neighbors. This setting does not refer to the actual size of the circle, however, it refers to how many neighboring points are to fall inside of the circle.\n",
    "\n",
    "We will start our modeling with the KNeighborsClassifier with the default number of neighbors (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "knn_results = baseline_report(knn, X_train, X_test, y_train, y_test, 'KNeighborsClassifier')\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus our hyperparameter tuning to optimize the **F1_score**. When optimizing for F1_score you take both precision and recall into account. If you try to only optimize recall, your algorithm will predict most examples to belong to the positive class, but that will result in many false positives and, hence, low precision. On the other hand, if you try to optimize precision, your model will predict very few examples as positive results (the ones which highest probability), but recall will be very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a GridSearchCV method, it's exostive and would be difficult to run on big datasets, but in our case it will work very well. Like that we will evaluate all possible cases of different parameter values and will find the best selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on the number of neighbors: It's best to choose the odd number for the n_neighbors. You can choose an even number, but in the case of a tie vote, the decision on which class to assign will be done randomly when weights is set to uniform. By choosing an odd number, there are no ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to mention is that n_jobs = -1. By setting n_jobs to -1, you’re telling the computer to use all of it’s processors to perform the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': [5,7,9,11,13,15,17,19],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'metric' : ['euclidean', 'manhattan']}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_params_knn = grid.best_params_\n",
    "print(best_params_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "knn_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'KNeighborsClassifier')\n",
    "knn_results_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T09:23:54.157705Z",
     "start_time": "2020-05-08T09:23:54.148684Z"
    }
   },
   "source": [
    "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "logreg_results = baseline_report(logreg, X_train, X_test, y_train, y_test, 'LogisticRegression')\n",
    "logreg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = list(10.0**np.arange(-3, 3))\n",
    "\n",
    "param_grid = [{'C': c_values,\n",
    "              'penalty': ['l2'],\n",
    "              'solver' : ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "              'multi_class': ['ovr', 'warn']},\n",
    "              {'C': c_values,\n",
    "              'penalty': ['l1'],\n",
    "              'solver' : ['liblinear'],\n",
    "              'multi_class': ['ovr', 'warn']}]\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_params_logreg = grid.best_params_\n",
    "print(best_params_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "logreg_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'LogisticRegression')\n",
    "logreg_results_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = dectree.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "dectree_results = baseline_report(dectree, X_train, X_test, y_train, y_test, 'DecisionTreeClassifier')\n",
    "dectree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're will be using a defined function to visualize the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(dectree, df_encoded)\n",
    "plt.savefig('feature_importance_dectree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**criterion** : optional (default=”gini”) or Choose attribute selection measure: This parameter allows us to use the different attribute selection measure. Supported criteria are “gini” for the Gini index and “entropy” for the information gain.\n",
    "\n",
    "**splitter** : string, optional (default=”best”) or Split Strategy: This parameter allows us to choose the split strategy. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "\n",
    "**max_depth** : int or None, optional (default=None) or Maximum Depth of a Tree: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = list(np.arange(2, 10))\n",
    "\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'splitter': ['best', 'random'],\n",
    "              'max_depth' : depth}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_dectree = grid.best_params_\n",
    "print(best_params_dectree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "dectree_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'DecisionTreeClassifier')\n",
    "dectree_results_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfor = RandomForestClassifier()\n",
    "rfor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = rfor.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "rfor_results = baseline_report(rfor, X_train, X_test, y_train, y_test, 'RandomForestClassifier')\n",
    "rfor_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(rfor, df_encoded)\n",
    "plt.savefig('feature_importance_rfor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results\n",
    "\n",
    "# can choose to split where we see elbow or after random\n",
    "\n",
    "#strange that anypage has such a low performance, while each page alone have higher.\n",
    "#check if it's done ok, if it's not redundant info\n",
    "#check what we put under browser 99 when did transformation, why other has higher importance??\n",
    "\n",
    "#explanation of alberto, to change\n",
    "#Similarly to the single decision tree, the random forest also gives a lot of importance to the “Glucose” feature, but it also chooses “BMI” to be the 2nd most informative feature overall. The randomness in building the random forest forces the algorithm to consider many possible explanations, the result being that the random forest captures a much broader picture of the data than a single tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [200, 500],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth' : [4,5,6,7,8],\n",
    "              'criterion' :['gini', 'entropy']}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_rfor = grid.best_params_\n",
    "print(best_params_rfor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "rfor_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'RandomForestClassifier')\n",
    "rfor_results_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linearsvc = LinearSVC()\n",
    "linearsvc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = linearsvc.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "linearsvc_results = baseline_report(linearsvc, X_train, X_test, y_train, y_test, 'LinearSVC')\n",
    "linearsvc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing results\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = list(10.0**np.arange(-3, 3))\n",
    "\n",
    "param_grid = [{'C': c_values,\n",
    "              'multi_class': ['ovr', 'crammer_singer'],\n",
    "              'penalty': ['l1'],\n",
    "              'loss': ['squared_hinge'],\n",
    "              'dual': [0]},\n",
    "              {'C': c_values,\n",
    "              'multi_class': ['ovr', 'crammer_singer'],\n",
    "              'penalty': ['l2'],\n",
    "              'loss': ['squared_hinge'],\n",
    "              'dual': [1]}]\n",
    "\n",
    "grid = GridSearchCV(LinearSVC(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_linearsvc = grid.best_params_\n",
    "print(best_params_linearsvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "linearsvc_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'LinearSVC')\n",
    "linearsvc_results_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_b = GaussianNB()\n",
    "naive_b.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = naive_b.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "naive_b_results = baseline_report(naive_b, X_train, X_test, y_train, y_test, 'GaussianNB')\n",
    "naive_b_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that we will be checking:\n",
    "\n",
    "**var_smoothing** : float, optional (default=1e-9). Portion of the largest variance of all features that is added to variances for calculation stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = list(10.0**np.arange(-9, -4))\n",
    "\n",
    "param_grid = {'var_smoothing' : smooth}\n",
    "\n",
    "grid = GridSearchCV(GaussianNB(), param_grid, verbose=1, cv=3, n_jobs=-1, scoring='f1')                 \n",
    "grid.fit(X_train, y_train)  \n",
    "\n",
    "best_params_naive_b = grid.best_params_\n",
    "print(best_params_naive_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to evaluate our model with the selection of the best parameters and see the impact on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "print(f'Confussion Matrix for test data:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
    "print(f'\\nClassification report for test data:\\n{classification_report(y_test, y_pred_test)}')\n",
    "naive_b_results_hyp = baseline_report(grid.best_estimator_, X_train, X_test, y_train, y_test, 'GaussianNB')\n",
    "naive_b_results_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results before hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_hyp = [knn_results, logreg_results, dectree_results, rfor_results, \n",
    "              linearsvc_results, naive_b_results]\n",
    "\n",
    "final_results_before = pd.concat(before_hyp)\n",
    "final_results_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results after hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_hyp = [knn_results_hyp, logreg_results_hyp, dectree_results_hyp, rfor_results_hyp, \n",
    "              linearsvc_results_hyp, naive_b_results_hyp]\n",
    "\n",
    "final_results_after = pd.concat(after_hyp)\n",
    "final_results_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cross validation dataframe\n",
    "grid.cv_results_\n",
    "hyp_res = pd.DataFrame(grid.cv_results_)\n",
    "hyp_res\n",
    "hyp_res[['param_criterion', 'param_max_depth', 'param_max_features','param_n_estimators', \n",
    "         'mean_test_score']].sort_values('mean_test_score', ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what else I can take from it\n",
    "\n",
    "dir(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same cross validation done manually, check the difference with function\n",
    "\n",
    "#Cross validation with best_estimator from grid search (using strat_k_fold, by default have kfold)\n",
    "\n",
    "strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "initial_score = cross_val_score(grid.best_estimator_, X_test, y_test, cv=strat_k_fold, \n",
    "                                scoring='f1').mean()\n",
    "print(\"Final accuracy : {} \".format(initial_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miltiple scoring, should give the same as function, check the difference in settings\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "initial_score = cross_validate(grid.best_estimator_, X_test, y_test, cv=strat_k_fold, \n",
    "                               scoring=('f1', 'accuracy'))\n",
    "print(initial_score['test_f1'].mean())\n",
    "print(initial_score['test_accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV has a very high computation cost if we want to try a bigger range of parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV to randomly try different parameters, but in our case the dataset is not \n",
    "#too big so we can allow to run gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter for all models to find all best parameter values for all models \n",
    "# https://www.youtube.com/watch?v=HdlDYng8g9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
